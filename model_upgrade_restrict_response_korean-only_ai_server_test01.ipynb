{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd3691c-94d1-4b66-b6f0-ff884e60f699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4236e97-a626-4a81-b988-8f51df6c444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (0.116.1)\n",
      "Requirement already satisfied: uvicorn in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (0.35.0)\n",
      "Requirement already satisfied: python-multipart in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (0.0.20)\n",
      "Requirement already satisfied: moviepy in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: openai-whisper in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (20250625)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from fastapi) (0.47.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from fastapi) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from fastapi) (4.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from uvicorn) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from moviepy) (5.2.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from moviepy) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from moviepy) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.25.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from moviepy) (2.1.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from moviepy) (0.1.12)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from moviepy) (1.1.1)\n",
      "Requirement already satisfied: pillow<12.0,>=9.2.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from moviepy) (11.2.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from proglog<=1.0.0->moviepy) (4.67.1)\n",
      "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from openai-whisper) (10.7.0)\n",
      "Requirement already satisfied: numba in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from openai-whisper) (0.61.2)\n",
      "Requirement already satisfied: tiktoken in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from openai-whisper) (0.11.0)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from openai-whisper) (2.6.0+cu118)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from numba->openai-whisper) (0.44.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.6.15)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from torch->openai-whisper) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from torch->openai-whisper) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from torch->openai-whisper) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\pytorch_env01\\lib\\site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn python-multipart moviepy openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b3365-4984-4bed-85b3-96b247e09897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper Large 모델 로딩 중... (시간이 걸릴 수 있습니다)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [12404]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로딩 완료! 디바이스: cuda\n",
      "서버 시작 중...\n",
      "INFO:     127.0.0.1:51099 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:51099 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "process_video 시작\n",
      "Whisper 변환 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 592/592 [00:02<00:00, 293.44frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:51114 - \"POST /process_video HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 한국어 최적화된 STT 서버 (Large 모델 + 추가 최적화)\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from moviepy import VideoFileClip\n",
    "import whisper\n",
    "import torch\n",
    "import tempfile\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "import traceback\n",
    "import subprocess\n",
    "import os\n",
    "import gc\n",
    "app = FastAPI()\n",
    "# GPU 메모리 최적화\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "# Large 모델 로딩 (더 정확하지만 느림)\n",
    "print(\"Whisper Large 모델 로딩 중... (시간이 걸릴 수 있습니다)\")\n",
    "model = whisper.load_model(\"large\", device=device)\n",
    "print(f\"모델 로딩 완료! 디바이스: {device}\")\n",
    "@app.post(\"/process_video\")\n",
    "async def process_video(file: UploadFile = File(...)):\n",
    "    print(\"process_video 시작\")\n",
    "    try:\n",
    "        # 업로드 파일명/확장자 확인\n",
    "        filename = (file.filename or \"\").lower()\n",
    "        if not filename.endswith((\".mp4\", \".webm\", \".avi\", \".mov\")):\n",
    "            return {\"status\": \"error\", \"message\": \"mp4, webm, avi, mov 파일만 지원합니다.\"}\n",
    "        ext = filename.split('.')[-1]\n",
    "        ext = f\".{ext}\"\n",
    "        # 업로드 파일을 메모리에서 읽기\n",
    "        video_bytes = await file.read()\n",
    "        # 파일 크기 제한 (100MB)\n",
    "        if len(video_bytes) > 100 * 1024 * 1024:\n",
    "            return {\"status\": \"error\", \"message\": \"파일 크기는 100MB 이하여야 합니다.\"}\n",
    "        # 임시로 업로드 확장자에 맞춰 저장\n",
    "        with tempfile.NamedTemporaryFile(suffix=ext, delete=False) as temp_video:\n",
    "            temp_video.write(video_bytes)\n",
    "            video_path = temp_video.name\n",
    "        audio_path = None\n",
    "        # 1) MoviePy로 오디오 추출 시도\n",
    "        try:\n",
    "            with VideoFileClip(video_path) as video_clip:\n",
    "                audio_clip = video_clip.audio\n",
    "                if audio_clip is None:\n",
    "                    if os.path.exists(video_path):\n",
    "                        os.unlink(video_path)\n",
    "                    return {\"status\": \"error\", \"message\": \"동영상에서 음성을 찾을 수 없습니다.\"}\n",
    "                # Whisper 최적화 오디오 포맷\n",
    "                with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_audio:\n",
    "                    audio_path = temp_audio.name\n",
    "                audio_clip.write_audiofile(\n",
    "                    audio_path,\n",
    "                    codec=\"pcm_s16le\",  # PCM 16bit\n",
    "                    fps=16000,          # 16kHz (Whisper 최적)\n",
    "                    ffmpeg_params=[\"-ac\", \"1\"],  # mono\n",
    "                    verbose=False,\n",
    "                    logger=None,\n",
    "                )\n",
    "                audio_clip.close()\n",
    "        except Exception as moviepy_err:\n",
    "            # 2) ffmpeg 백업 방식\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_audio:\n",
    "                audio_path = temp_audio.name\n",
    "            cmd = [\n",
    "                \"ffmpeg\", \"-y\", \"-i\", video_path,\n",
    "                \"-vn\",                    # 비디오 제거\n",
    "                \"-acodec\", \"pcm_s16le\",  # PCM s16\n",
    "                \"-ar\", \"16000\",          # 16kHz\n",
    "                \"-ac\", \"1\",              # mono\n",
    "                \"-af\", \"volume=1.5\",     # 볼륨 약간 증가\n",
    "                audio_path,\n",
    "            ]\n",
    "            try:\n",
    "                result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "            except subprocess.CalledProcessError as ffmpeg_err:\n",
    "                # 임시 파일 정리\n",
    "                if os.path.exists(video_path):\n",
    "                    os.unlink(video_path)\n",
    "                if os.path.exists(audio_path):\n",
    "                    os.unlink(audio_path)\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"오디오 추출 실패: {moviepy_err}\",\n",
    "                }\n",
    "        # 3) Whisper로 고품질 한국어 변환\n",
    "        print(\"Whisper 변환 시작...\")\n",
    "        # GPU 메모리 정리\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        result = model.transcribe(\n",
    "            audio_path,\n",
    "            language=\"ko\",                    # 한국어 지정\n",
    "            fp16=(device == \"cuda\"),         # GPU 사용시 FP16\n",
    "            temperature=0.0,                 # 더 일관된 결과\n",
    "            beam_size=5,                     # 빔 서치 크기 증가\n",
    "            best_of=5,                       # 최고 결과 선택\n",
    "            no_speech_threshold=0.6,         # 무음 구간 임계값\n",
    "            logprob_threshold=-1.0,          # 로그 확률 임계값\n",
    "            compression_ratio_threshold=2.4,  # 압축 비율 임계값\n",
    "            condition_on_previous_text=True, # 이전 텍스트 조건부\n",
    "            verbose=False,\n",
    "        )\n",
    "        transcribed_text = (result.get(\"text\") or \"\").strip()\n",
    "        # 한국어 후처리\n",
    "        if transcribed_text:\n",
    "            import re\n",
    "            # 한글, 공백, 숫자, 기본 문장부호만 허용\n",
    "            filtered_text = re.sub(r'[^\\uAC00-\\uD7A3\\s0-9.,!?~\\-()\"]', '', transcribed_text)\n",
    "            # 연속 공백 제거\n",
    "            filtered_text = re.sub(r'\\s+', ' ', filtered_text)\n",
    "            transcribed_text = filtered_text.strip()\n",
    "        # 세그먼트 정보도 함께 반환 (선택사항)\n",
    "        segments = []\n",
    "        if result.get(\"segments\"):\n",
    "            for segment in result[\"segments\"][:5]:  # 처음 5개만\n",
    "                segments.append({\n",
    "                    \"start\": round(segment.get(\"start\", 0), 2),\n",
    "                    \"end\": round(segment.get(\"end\", 0), 2),\n",
    "                    \"text\": segment.get(\"text\", \"\").strip()\n",
    "                })\n",
    "        # 임시 파일 정리\n",
    "        if os.path.exists(video_path):\n",
    "            os.unlink(video_path)\n",
    "        if audio_path and os.path.exists(audio_path):\n",
    "            os.unlink(audio_path)\n",
    "        # 메모리 정리\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # 결과 반환\n",
    "        response = {\n",
    "            \"status\": \"success\",\n",
    "            \"transcription\": transcribed_text,\n",
    "            \"language\": \"korean\",\n",
    "            \"model\": \"whisper-large\"\n",
    "        }\n",
    "        # 세그먼트 정보 추가 (선택사항)\n",
    "        if segments:\n",
    "            response[\"segments\"] = segments\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        error_traceback = traceback.format_exc()\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Traceback: {error_traceback}\")\n",
    "        # 예외 발생 시 임시 파일 정리\n",
    "        try:\n",
    "            if 'video_path' in locals() and os.path.exists(video_path):\n",
    "                os.unlink(video_path)\n",
    "            if 'audio_path' in locals() and audio_path and os.path.exists(audio_path):\n",
    "                os.unlink(audio_path)\n",
    "        except:\n",
    "            pass\n",
    "        # GPU 메모리 정리\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": str(e),\n",
    "            \"traceback\": error_traceback\n",
    "        }\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model\": \"whisper-large\",\n",
    "        \"language\": \"korean\",\n",
    "        \"device\": device\n",
    "    }\n",
    "@app.get(\"/model-info\")\n",
    "async def model_info():\n",
    "    return {\n",
    "        \"model_name\": \"whisper-large\",\n",
    "        \"device\": device,\n",
    "        \"language\": \"korean\",\n",
    "        \"supported_formats\": [\"mp4\", \"webm\", \"avi\", \"mov\"],\n",
    "        \"max_file_size\": \"100MB\"\n",
    "    }\n",
    "# Jupyter notebook에서 서버 실행\n",
    "# 서버 시작 후 다음 주소로 접속:\n",
    "# - API 문서: http://localhost:8000/docs\n",
    "# - 헬스체크: http://localhost:8000/health\n",
    "# - 모델정보: http://localhost:8000/model-info\n",
    "#\n",
    "# 사용 방법:\n",
    "# 1. http://localhost:8000/docs 접속\n",
    "# 2. /process_video 섹션에서 \"Try it out\" 버튼 클릭\n",
    "# 3. \"Choose File\"로 MP4/WebM 파일 선택\n",
    "# 4. \"Execute\" 버튼 클릭\n",
    "# 5. Response body에서 한국어 변환 결과 확인\n",
    "\n",
    "try:\n",
    "    nest_asyncio.apply()\n",
    "    print(\"서버 시작 중...\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, http='h11')\n",
    "except RuntimeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a611f1-8f8a-4083-b7c2-1487871bb930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
