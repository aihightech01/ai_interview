{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df6686-2d6e-4a60-8709-f826ecbadd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import tempfile\n",
    "import math\n",
    "import random\n",
    "from typing import Dict\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi import Form\n",
    "\n",
    "from l2cs import Pipeline\n",
    "\n",
    "# --- 시드 고정 코드 ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # CUDA의 비결정적 연산을 최대한 방지 (성능이 약간 저하될 수 있음)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# --------------------\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. FastAPI 애플리케이션 및 AI 모델 초기화 (변경 없음)\n",
    "# ==============================================================================\n",
    "print(\"FastAPI 서버를 시작합니다. AI 모델을 로딩합니다...\")\n",
    "app = FastAPI(title=\"AI Interview Analysis Server\")\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "try:\n",
    "    gaze_pipeline = Pipeline(\n",
    "        weights='models/L2CSNet_gaze360.pkl', arch='ResNet50', device=torch.device(device)\n",
    "    )\n",
    "    print(f\"AI 모델 로딩 완료. Device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading L2CS-Net model: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FastAPI 엔드포인트 정의 (내부 로직 수정)\n",
    "# ==============================================================================\n",
    "@app.post(\"/calibrate\", response_model=Dict[str, float])\n",
    "async def analyze_video(video_file: UploadFile = File(...)):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_video:\n",
    "            contents = await video_file.read()\n",
    "            temp_video.write(contents)\n",
    "            temp_video_path = temp_video.name\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"임시 파일 생성에 실패했습니다: {e}\")\n",
    "\n",
    "    head_yaws, head_pitches, gaze_yaws, gaze_pitches = [], [], [], []\n",
    "    cap = cv2.VideoCapture(temp_video_path)\n",
    "    if not cap.isOpened():\n",
    "        os.unlink(temp_video_path)\n",
    "        raise HTTPException(status_code=400, detail=\"업로드된 비디오 파일을 열 수 없거나 손상되었습니다.\")\n",
    "\n",
    "    # ( ★★★ 1. 프레임 카운터 초기화 ★★★ )\n",
    "    # 분석 결과를 저장할 리스트 초기화\n",
    "    head_yaws, head_pitches = [], []\n",
    "    gaze_yaws, gaze_pitches = [], []\n",
    "\n",
    "    # --- EMA 필터를 위한 변수 초기화 ---\n",
    "    # 첫 프레임의 값을 저장하기 위해 None으로 초기화합니다.\n",
    "    smooth_head_yaw, smooth_head_pitch = None, None\n",
    "    smooth_gaze_yaw, smooth_gaze_pitch = None, None\n",
    "    alpha = 0.2 # 스무딩 강도 (이 값을 조절하여 부드러움을 변경할 수 있습니다)\n",
    "    # -----------------------------------\n",
    "    \n",
    "    frame_number = 0\n",
    "    \n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            # ( ★★★ 2. 모든 프레임에 대해 카운터 증가 ★★★ )\n",
    "            frame_number += 1\n",
    "\n",
    "            # ( ★★★ 3. 10번째 프레임이 아니면 건너뛰기 ★★★ )\n",
    "            # frame_number를 10으로 나눈 나머지가 0일 때만 아래 분석 로직을 실행합니다.\n",
    "            if frame_number % 10 != 0:\n",
    "                continue\n",
    "\n",
    "            # --- [머리 방향 추정 로직] --- (10프레임마다 실행됨)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results_facemesh = face_mesh.process(rgb_frame)\n",
    "            if results_facemesh.multi_face_landmarks:\n",
    "                face_landmarks = results_facemesh.multi_face_landmarks[0]\n",
    "                img_h, img_w, _ = frame.shape\n",
    "                face_3d_model = np.array([[0.0, 0.0, 0.0], [0.0, -330.0, -65.0], [-225.0, 170.0, -135.0], [225.0, 170.0, -135.0], [-150.0, -150.0, -125.0], [150.0, -150.0, -125.0]], dtype=np.float64)\n",
    "                landmark_idx = [1, 152, 263, 33, 291, 61]\n",
    "                face_2d_points = np.array([ [face_landmarks.landmark[idx].x * img_w, face_landmarks.landmark[idx].y * img_h] for idx in landmark_idx ], dtype=np.float64)\n",
    "                focal_length = img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2], [0, focal_length, img_h / 2], [0, 0, 1]])\n",
    "                dist_coeffs = np.zeros((4, 1), dtype=np.float64)\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d_model, face_2d_points, cam_matrix, dist_coeffs)\n",
    "                rot_mat, _ = cv2.Rodrigues(rot_vec)\n",
    "                sy = np.sqrt(rot_mat[0, 0] * rot_mat[0, 0] + rot_mat[1, 0] * rot_mat[1, 0])\n",
    "                singular = sy < 1e-6\n",
    "                if not singular:\n",
    "                    x, y, _ = np.arctan2(rot_mat[2, 1], rot_mat[2, 2]), np.arctan2(-rot_mat[2, 0], sy), np.arctan2(rot_mat[1, 0], rot_mat[0, 0])\n",
    "                else:\n",
    "                    x, y, _ = np.arctan2(-rot_mat[1, 2], rot_mat[1, 1]), np.arctan2(-rot_mat[2, 0], sy), 0\n",
    "                # 1. 원본(날것)의 각도 값 계산\n",
    "                head_pitch = -np.degrees(x)\n",
    "                head_yaw = -np.degrees(y)\n",
    "\n",
    "                # 2. EMA 필터 적용\n",
    "                if smooth_head_yaw is None: # 첫 프레임인 경우\n",
    "                    smooth_head_yaw = head_yaw\n",
    "                    smooth_head_pitch = head_pitch\n",
    "                else:\n",
    "                    smooth_head_yaw = alpha * head_yaw + (1 - alpha) * smooth_head_yaw\n",
    "                    smooth_head_pitch = alpha * head_pitch + (1 - alpha) * smooth_head_pitch\n",
    "\n",
    "                # 3. 부드러워진 값을 리스트에 추가\n",
    "                head_pitches.append(smooth_head_pitch)\n",
    "                head_yaws.append(smooth_head_yaw)\n",
    "\n",
    "            # --- [시선 추정 로직] ---\n",
    "            results_gaze = gaze_pipeline.step(frame)\n",
    "            if results_gaze and results_gaze.pitch is not None and len(results_gaze.pitch) > 0:\n",
    "                # 1. 원본(날것)의 각도 값 계산\n",
    "                gaze_pitch = results_gaze.pitch[0]\n",
    "                gaze_yaw = results_gaze.yaw[0]\n",
    "                \n",
    "                # 2. EMA 필터 적용\n",
    "                if smooth_gaze_yaw is None: # 첫 프레임인 경우\n",
    "                    smooth_gaze_yaw = gaze_yaw\n",
    "                    smooth_gaze_pitch = gaze_pitch\n",
    "                else:\n",
    "                    smooth_gaze_yaw = alpha * gaze_yaw + (1 - alpha) * smooth_gaze_yaw\n",
    "                    smooth_gaze_pitch = alpha * gaze_pitch + (1 - alpha) * smooth_gaze_pitch\n",
    "                    \n",
    "                # 3. 부드러워진 값을 리스트에 추가\n",
    "                gaze_pitches.append(smooth_gaze_pitch)\n",
    "                gaze_yaws.append(smooth_gaze_yaw)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"비디오 분석 중 오류가 발생했습니다: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        os.unlink(temp_video_path)\n",
    "\n",
    "    # --- [평균값 계산 및 결과 반환] --- (변경 없음)\n",
    "    avg_head_yaw = sum(head_yaws) / len(head_yaws) if head_yaws else 0.0\n",
    "    avg_head_pitch = sum(head_pitches) / len(head_pitches) if head_pitches else 0.0\n",
    "    avg_gaze_yaw = sum(gaze_yaws) / len(gaze_yaws) if gaze_yaws else 0.0\n",
    "    avg_gaze_pitch = sum(gaze_pitches) / len(gaze_pitches) if gaze_pitches else 0.0\n",
    "    \n",
    "    # Spring Boot의 CalibrationResultDto의 @JsonProperty에 맞춰 key 값을 변경합니다.\n",
    "    response_content = {\n",
    "        \"head_yaw\": float(avg_head_yaw),     # \"avg_head_yaw\" -> \"head_yaw\"\n",
    "        \"head_pitch\": float(avg_head_pitch), # \"avg_head_pitch\" -> \"head_pitch\"\n",
    "        \"gaze_yaw\": float(avg_gaze_yaw),     # \"avg_gaze_yaw\" -> \"gaze_yaw\"\n",
    "        \"gaze_pitch\": float(avg_gaze_pitch)  # \"avg_gaze_pitch\" -> \"gaze_pitch\"\n",
    "    }\n",
    "    return JSONResponse(content=response_content)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 면접 영상 분석 엔드포인트 (/analyze_video) - 새로 추가된 기능\n",
    "# ==============================================================================\n",
    "@app.post(\"/analyze_video\")\n",
    "async def analyze_interview_video(video_file: UploadFile = File(...),\n",
    "                                  calib_hy: float = Form(...),\n",
    "                                  calib_hp: float = Form(...),\n",
    "                                  calib_gy: float = Form(...),\n",
    "                                  calib_gp: float = Form(...)):\n",
    "    \"\"\"\n",
    "    업로드된 면접 동영상을 10프레임 단위로 분석하고,\n",
    "    EMA 필터를 적용한 시계열 데이터를 JSON 배열로 반환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_video:\n",
    "            contents = await video_file.read()\n",
    "            temp_video.write(contents)\n",
    "            temp_video_path = temp_video.name\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"임시 파일 생성에 실패했습니다: {e}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(temp_video_path)\n",
    "    if not cap.isOpened():\n",
    "        os.unlink(temp_video_path)\n",
    "        raise HTTPException(status_code=400, detail=\"업로드된 비디오 파일을 열 수 없거나 손상되었습니다.\")\n",
    "\n",
    "    # --- 변수 초기화 ---\n",
    "    time_series_data = []\n",
    "    frame_number = 0\n",
    "    # EMA 필터를 위한 변수\n",
    "    smooth_head_yaw, smooth_head_pitch = None, None\n",
    "    smooth_gaze_yaw, smooth_gaze_pitch = None, None\n",
    "    alpha = 0.2  # 스무딩 강도\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            frame_number += 1\n",
    "            if frame_number % 10 != 0:\n",
    "                continue\n",
    "\n",
    "            # --- AI 분석 로직 (머리 + 시선) ---\n",
    "            head_yaw, head_pitch, gaze_yaw, gaze_pitch = None, None, None, None\n",
    "\n",
    "            # [머리 방향 추정]\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results_facemesh = face_mesh.process(rgb_frame)\n",
    "            if results_facemesh.multi_face_landmarks:\n",
    "                face_landmarks = results_facemesh.multi_face_landmarks[0]\n",
    "                # ... (solvePnP 및 각도 계산 로직은 이전과 동일)\n",
    "                img_h, img_w, _ = frame.shape\n",
    "                face_3d_model = np.array([[0.0, 0.0, 0.0], [0.0, -330.0, -65.0], [-225.0, 170.0, -135.0], [225.0, 170.0, -135.0], [-150.0, -150.0, -125.0], [150.0, -150.0, -125.0]], dtype=np.float64)\n",
    "                landmark_idx = [1, 152, 263, 33, 291, 61]\n",
    "                face_2d_points = np.array([ [face_landmarks.landmark[idx].x * img_w, face_landmarks.landmark[idx].y * img_h] for idx in landmark_idx ], dtype=np.float64)\n",
    "                focal_length = img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2], [0, focal_length, img_h / 2], [0, 0, 1]])\n",
    "                dist_coeffs = np.zeros((4, 1), dtype=np.float64)\n",
    "                success, rot_vec, trans_vec = cv2.solvePnP(face_3d_model, face_2d_points, cam_matrix, dist_coeffs)\n",
    "                rot_mat, _ = cv2.Rodrigues(rot_vec)\n",
    "                sy = np.sqrt(rot_mat[0, 0] * rot_mat[0, 0] + rot_mat[1, 0] * rot_mat[1, 0])\n",
    "                singular = sy < 1e-6\n",
    "                if not singular:\n",
    "                    x, y, _ = np.arctan2(rot_mat[2, 1], rot_mat[2, 2]), np.arctan2(-rot_mat[2, 0], sy), np.arctan2(rot_mat[1, 0], rot_mat[0, 0])\n",
    "                else:\n",
    "                    x, y, _ = np.arctan2(-rot_mat[1, 2], rot_mat[1, 1]), np.arctan2(-rot_mat[2, 0], sy), 0\n",
    "                \n",
    "                head_pitch = -np.degrees(x)\n",
    "                head_yaw = -np.degrees(y)\n",
    "\n",
    "            # [시선 추정]\n",
    "            results_gaze = gaze_pipeline.step(frame)\n",
    "            if results_gaze and results_gaze.pitch is not None and len(results_gaze.pitch) > 0:\n",
    "                gaze_pitch = results_gaze.pitch[0]\n",
    "                gaze_yaw = results_gaze.yaw[0]\n",
    "            \n",
    "            # --- EMA 필터 적용 및 데이터 저장 ---\n",
    "            # 분석에 성공한 경우에만 데이터 처리\n",
    "            if head_yaw is not None and gaze_yaw is not None:\n",
    "                if smooth_head_yaw is None: # 첫 분석 프레임인 경우\n",
    "                    smooth_head_yaw, smooth_head_pitch = head_yaw, head_pitch\n",
    "                    smooth_gaze_yaw, smooth_gaze_pitch = gaze_yaw, gaze_pitch\n",
    "                else:\n",
    "                    smooth_head_yaw = alpha * head_yaw + (1 - alpha) * smooth_head_yaw\n",
    "                    smooth_head_pitch = alpha * head_pitch + (1 - alpha) * smooth_head_pitch\n",
    "                    smooth_gaze_yaw = alpha * gaze_yaw + (1 - alpha) * smooth_gaze_yaw\n",
    "                    smooth_gaze_pitch = alpha * gaze_pitch + (1 - alpha) * smooth_gaze_pitch\n",
    "                \n",
    "                frame_data = {\n",
    "                    \"frame\": frame_number,\n",
    "                    \"head_yaw\": float(smooth_head_yaw),\n",
    "                    \"head_pitch\": float(smooth_head_pitch),\n",
    "                    \"gaze_yaw\": float(smooth_gaze_yaw),\n",
    "                    \"gaze_pitch\": float(smooth_gaze_pitch)\n",
    "                }\n",
    "                time_series_data.append(frame_data)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"비디오 분석 중 오류가 발생했습니다: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        os.unlink(temp_video_path)\n",
    "\n",
    "    scores = calc_score(time_series_data, calib_hy, calib_hp, calib_gy, calib_gp)\n",
    "    for frame_data, score in zip(time_series_data, scores):\n",
    "        frame_data[\"score\"] = score\n",
    "\n",
    "    # 평균 점수 계산\n",
    "    average_score = float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "    # 최종 반환: 타임시리즈 데이터와 평균 점수를 함께 반환\n",
    "    return JSONResponse(content={\n",
    "        \"time_series\": time_series_data,\n",
    "        \"average_score\": round(average_score)\n",
    "    })\n",
    "\n",
    "def calc_score(time_series, calib_hy, calib_hp, calib_gy, calib_gp):\n",
    "    scores = []\n",
    "    N = 9\n",
    "    K = 0.1\n",
    "    head_yaw_list, head_pitch_list, gaze_yaw_list, gaze_pitch_list = [], [], [], []\n",
    "    for idx, row in enumerate(time_series):\n",
    "        hy, hp, gy, gp = row[\"head_yaw\"], row[\"head_pitch\"], row[\"gaze_yaw\"], row[\"gaze_pitch\"]\n",
    "        head_yaw_list.append(hy)\n",
    "        head_pitch_list.append(hp)\n",
    "        gaze_yaw_list.append(gy)\n",
    "        gaze_pitch_list.append(gp)\n",
    "        # 초기 9개는 누적 평균, 이후부터는 이동 평균\n",
    "        if idx < N:\n",
    "            mean_hy = np.mean(head_yaw_list)\n",
    "            mean_hp = np.mean(head_pitch_list)\n",
    "            mean_gy = np.mean(gaze_yaw_list)\n",
    "            mean_gp = np.mean(gaze_pitch_list)\n",
    "        else:\n",
    "            mean_hy = np.mean(head_yaw_list[-N:])\n",
    "            mean_hp = np.mean(head_pitch_list[-N:])\n",
    "            mean_gy = np.mean(gaze_yaw_list[-N:])\n",
    "            mean_gp = np.mean(gaze_pitch_list[-N:])\n",
    "        abs_delta_hy = abs(hy - calib_hy)\n",
    "        abs_delta_hp = abs(hp - calib_hp)\n",
    "        abs_delta_gy = abs(gy - calib_gy)\n",
    "        abs_delta_gp = abs(gp - calib_gp)\n",
    "        rel_delta_hy = abs(hy - mean_hy)\n",
    "        rel_delta_hp = abs(hp - mean_hp)\n",
    "        rel_delta_gy = abs(gy - mean_gy)\n",
    "        rel_delta_gp = abs(gp - mean_gp)\n",
    "        w1, w2, w3, w4 = 0.25, 0.25, 0.25, 0.25\n",
    "        S = (\n",
    "            w1 * abs_delta_hy + w2 * abs_delta_hp + w3 * abs_delta_gy + w4 * abs_delta_gp +\n",
    "            w1 * rel_delta_hy + w2 * rel_delta_hp + w3 * rel_delta_gy + w4 * rel_delta_gp\n",
    "        )\n",
    "        score = int(100 * math.exp(-K * S))\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=5003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41a36d-912f-453e-badc-2e10f34a0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed fixed to 42\n",
      "FastAPI 서버를 시작합니다. AI 모델을 로딩합니다...\n",
      "AI 모델 로딩 완료. Device: cuda\n",
      "Starting uvicorn server on 0.0.0.0:5003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [16028]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5003 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received /calibrate request\n",
      "Creating temporary file for video\n",
      "Temporary video file saved at: C:\\Users\\User\\AppData\\Local\\Temp\\tmp9cd6gf4_.mp4\n",
      "Processing frame number: 10\n",
      "Face detected\n",
      "Raw head_pitch: -0.16, head_yaw: 25.73\n",
      "Smoothed head_pitch: -0.16, head_yaw: 25.73\n",
      "Raw gaze_pitch: -0.29, gaze_yaw: 0.28\n",
      "Smoothed gaze_pitch: -0.29, gaze_yaw: 0.28\n",
      "Processing frame number: 20\n",
      "Face detected\n",
      "Raw head_pitch: -3.43, head_yaw: 33.80\n",
      "Smoothed head_pitch: -0.82, head_yaw: 27.34\n",
      "Raw gaze_pitch: -0.03, gaze_yaw: 0.23\n",
      "Smoothed gaze_pitch: -0.23, gaze_yaw: 0.27\n",
      "Processing frame number: 30\n",
      "Face detected\n",
      "Raw head_pitch: -3.79, head_yaw: 33.25\n",
      "Smoothed head_pitch: -1.41, head_yaw: 28.52\n",
      "Raw gaze_pitch: -0.44, gaze_yaw: 0.37\n",
      "Smoothed gaze_pitch: -0.28, gaze_yaw: 0.29\n",
      "Video frame read complete or failed\n",
      "VideoCapture released\n",
      "Temporary file deleted: C:\\Users\\User\\AppData\\Local\\Temp\\tmp9cd6gf4_.mp4\n",
      "Returning response: {'head_yaw': 27.19860636313094, 'head_pitch': -0.7968247062894834, 'gaze_yaw': 0.27984469572703047, 'gaze_pitch': -0.2653657705088457}\n",
      "INFO:     127.0.0.1:58603 - \"POST /calibrate HTTP/1.1\" 200 OK\n",
      "Received /calibrate request\n",
      "Creating temporary file for video\n",
      "Temporary video file saved at: C:\\Users\\User\\AppData\\Local\\Temp\\tmpsj0p9zhf.mp4\n",
      "Processing frame number: 10\n",
      "Face detected\n",
      "Raw head_pitch: -0.16, head_yaw: -12.01\n",
      "Smoothed head_pitch: -0.16, head_yaw: -12.01\n",
      "Raw gaze_pitch: 0.76, gaze_yaw: 0.19\n",
      "Smoothed gaze_pitch: 0.76, gaze_yaw: 0.19\n",
      "Processing frame number: 20\n",
      "Face detected\n",
      "Raw head_pitch: -4.67, head_yaw: 25.92\n",
      "Smoothed head_pitch: -1.06, head_yaw: -4.42\n",
      "Raw gaze_pitch: -0.46, gaze_yaw: 0.16\n",
      "Smoothed gaze_pitch: 0.51, gaze_yaw: 0.19\n",
      "Processing frame number: 30\n",
      "Face detected\n",
      "Raw head_pitch: -4.97, head_yaw: 26.79\n",
      "Smoothed head_pitch: -1.84, head_yaw: 1.82\n",
      "Raw gaze_pitch: -0.25, gaze_yaw: 0.22\n",
      "Smoothed gaze_pitch: 0.36, gaze_yaw: 0.19\n",
      "Processing frame number: 40\n",
      "Face detected\n",
      "Raw head_pitch: -4.57, head_yaw: 24.92\n",
      "Smoothed head_pitch: -2.39, head_yaw: 6.44\n",
      "Raw gaze_pitch: -0.32, gaze_yaw: 0.18\n",
      "Smoothed gaze_pitch: 0.23, gaze_yaw: 0.19\n",
      "Video frame read complete or failed\n",
      "VideoCapture released\n",
      "Temporary file deleted: C:\\Users\\User\\AppData\\Local\\Temp\\tmpsj0p9zhf.mp4\n",
      "Returning response: {'head_yaw': -2.042770912678562, 'head_pitch': -1.3599154550131836, 'gaze_yaw': 0.19153828659653666, 'gaze_pitch': 0.4656553092896939}\n",
      "INFO:     127.0.0.1:65379 - \"POST /calibrate HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import tempfile\n",
    "import math\n",
    "import random\n",
    "from typing import Dict\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from fastapi import Form\n",
    "\n",
    "from l2cs import Pipeline\n",
    "\n",
    "# --- 시드 고정 코드 ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "print(\"Seed fixed to 42\")\n",
    "\n",
    "print(\"FastAPI 서버를 시작합니다. AI 모델을 로딩합니다...\")\n",
    "app = FastAPI(title=\"AI Interview Analysis Server\")\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "try:\n",
    "    gaze_pipeline = Pipeline(\n",
    "        weights='models/L2CSNet_gaze360.pkl', arch='ResNet50', device=torch.device(device)\n",
    "    )\n",
    "    print(f\"AI 모델 로딩 완료. Device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading L2CS-Net model: {e}\")\n",
    "\n",
    "@app.post(\"/calibrate\", response_model=Dict[str, float])\n",
    "async def analyze_video(video_file: UploadFile = File(...)):\n",
    "    print(\"Received /calibrate request\")\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_video:\n",
    "            print(\"Creating temporary file for video\")\n",
    "            contents = await video_file.read()\n",
    "            temp_video.write(contents)\n",
    "            temp_video_path = temp_video.name\n",
    "            print(f\"Temporary video file saved at: {temp_video_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating temp file: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"임시 파일 생성에 실패했습니다: {e}\")\n",
    "\n",
    "    head_yaws, head_pitches, gaze_yaws, gaze_pitches = [], [], [], []\n",
    "    cap = cv2.VideoCapture(temp_video_path)\n",
    "    if not cap.isOpened():\n",
    "        os.unlink(temp_video_path)\n",
    "        print(\"Failed to open video file\")\n",
    "        raise HTTPException(status_code=400, detail=\"업로드된 비디오 파일을 열 수 없거나 손상되었습니다.\")\n",
    "\n",
    "    smooth_head_yaw, smooth_head_pitch = None, None\n",
    "    smooth_gaze_yaw, smooth_gaze_pitch = None, None\n",
    "    alpha = 0.2\n",
    "    frame_number = 0\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Video frame read complete or failed\")\n",
    "                break\n",
    "\n",
    "            frame_number += 1\n",
    "            if frame_number % 10 != 0:\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing frame number: {frame_number}\")\n",
    "\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results_facemesh = face_mesh.process(rgb_frame)\n",
    "            if results_facemesh.multi_face_landmarks:\n",
    "                print(\"Face detected\")\n",
    "                face_landmarks = results_facemesh.multi_face_landmarks[0]\n",
    "                img_h, img_w, _ = frame.shape\n",
    "\n",
    "                face_3d_model = np.array([[0.0, 0.0, 0.0], [0.0, -330.0, -65.0], [-225.0, 170.0, -135.0],\n",
    "                                         [225.0, 170.0, -135.0], [-150.0, -150.0, -125.0], [150.0, -150.0, -125.0]], dtype=np.float64)\n",
    "                landmark_idx = [1, 152, 263, 33, 291, 61]\n",
    "                face_2d_points = np.array(\n",
    "                    [[face_landmarks.landmark[idx].x * img_w, face_landmarks.landmark[idx].y * img_h] for idx in landmark_idx],\n",
    "                    dtype=np.float64)\n",
    "                focal_length = img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2], [0, focal_length, img_h / 2], [0, 0, 1]])\n",
    "                dist_coeffs = np.zeros((4, 1), dtype=np.float64)\n",
    "                success_pose, rot_vec, trans_vec = cv2.solvePnP(face_3d_model, face_2d_points, cam_matrix, dist_coeffs)\n",
    "                if not success_pose:\n",
    "                    print(\"solvePnP failed\")\n",
    "                    continue\n",
    "                rot_mat, _ = cv2.Rodrigues(rot_vec)\n",
    "                sy = np.sqrt(rot_mat[0, 0] ** 2 + rot_mat[1, 0] ** 2)\n",
    "                singular = sy < 1e-6\n",
    "                if not singular:\n",
    "                    x, y, _ = np.arctan2(rot_mat[2, 1], rot_mat[2, 2]), np.arctan2(-rot_mat[2, 0], sy), np.arctan2(rot_mat[1, 0], rot_mat[0, 0])\n",
    "                else:\n",
    "                    x, y, _ = np.arctan2(-rot_mat[1, 2], rot_mat[1, 1]), np.arctan2(-rot_mat[2, 0], sy), 0\n",
    "\n",
    "                head_pitch = -np.degrees(x)\n",
    "                head_yaw = -np.degrees(y)\n",
    "                print(f\"Raw head_pitch: {head_pitch:.2f}, head_yaw: {head_yaw:.2f}\")\n",
    "\n",
    "                if smooth_head_yaw is None:\n",
    "                    smooth_head_yaw = head_yaw\n",
    "                    smooth_head_pitch = head_pitch\n",
    "                else:\n",
    "                    smooth_head_yaw = alpha * head_yaw + (1 - alpha) * smooth_head_yaw\n",
    "                    smooth_head_pitch = alpha * head_pitch + (1 - alpha) * smooth_head_pitch\n",
    "                print(f\"Smoothed head_pitch: {smooth_head_pitch:.2f}, head_yaw: {smooth_head_yaw:.2f}\")\n",
    "\n",
    "                head_pitches.append(smooth_head_pitch)\n",
    "                head_yaws.append(smooth_head_yaw)\n",
    "            else:\n",
    "                print(f\"No face detected in frame {frame_number}\")\n",
    "\n",
    "            try:\n",
    "                results_gaze = gaze_pipeline.step(frame)\n",
    "                if results_gaze and results_gaze.pitch is not None and len(results_gaze.pitch) > 0:\n",
    "                    gaze_pitch = results_gaze.pitch[0]\n",
    "                    gaze_yaw = results_gaze.yaw[0]\n",
    "                    print(f\"Raw gaze_pitch: {gaze_pitch:.2f}, gaze_yaw: {gaze_yaw:.2f}\")\n",
    "\n",
    "                    if smooth_gaze_yaw is None:\n",
    "                        smooth_gaze_yaw = gaze_yaw\n",
    "                        smooth_gaze_pitch = gaze_pitch\n",
    "                    else:\n",
    "                        smooth_gaze_yaw = alpha * gaze_yaw + (1 - alpha) * smooth_gaze_yaw\n",
    "                        smooth_gaze_pitch = alpha * gaze_pitch + (1 - alpha) * smooth_gaze_pitch\n",
    "                    print(f\"Smoothed gaze_pitch: {smooth_gaze_pitch:.2f}, gaze_yaw: {smooth_gaze_yaw:.2f}\")\n",
    "\n",
    "                    gaze_pitches.append(smooth_gaze_pitch)\n",
    "                    gaze_yaws.append(smooth_gaze_yaw)\n",
    "                else:\n",
    "                    print(f\"Gaze data invalid or empty for frame {frame_number}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during gaze pipeline processing: {e}\")\n",
    "                raise HTTPException(status_code=500, detail=f\"Gaze 분석 중 오류가 발생했습니다: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during video analysis: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"비디오 분석 중 오류가 발생했습니다: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        print(\"VideoCapture released\")\n",
    "        try:\n",
    "            os.unlink(temp_video_path)\n",
    "            print(f\"Temporary file deleted: {temp_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting temporary file: {e}\")\n",
    "\n",
    "    avg_head_yaw = sum(head_yaws) / len(head_yaws) if head_yaws else 0.0\n",
    "    avg_head_pitch = sum(head_pitches) / len(head_pitches) if head_pitches else 0.0\n",
    "    avg_gaze_yaw = sum(gaze_yaws) / len(gaze_yaws) if gaze_yaws else 0.0\n",
    "    avg_gaze_pitch = sum(gaze_pitches) / len(gaze_pitches) if gaze_pitches else 0.0\n",
    "\n",
    "    response_content = {\n",
    "        \"head_yaw\": float(avg_head_yaw),\n",
    "        \"head_pitch\": float(avg_head_pitch),\n",
    "        \"gaze_yaw\": float(avg_gaze_yaw),\n",
    "        \"gaze_pitch\": float(avg_gaze_pitch)\n",
    "    }\n",
    "    print(f\"Returning response: {response_content}\")\n",
    "    return JSONResponse(content=response_content)\n",
    "\n",
    "@app.post(\"/analyze_video\")\n",
    "async def analyze_interview_video(video_file: UploadFile = File(...),\n",
    "                                  calib_hy: float = Form(...),\n",
    "                                  calib_hp: float = Form(...),\n",
    "                                  calib_gy: float = Form(...),\n",
    "                                  calib_gp: float = Form(...)):\n",
    "    print(\"Received /analyze_video request\")\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_video:\n",
    "            print(\"Creating temporary file for analysis video\")\n",
    "            contents = await video_file.read()\n",
    "            temp_video.write(contents)\n",
    "            temp_video_path = temp_video.name\n",
    "            print(f\"Temporary video file path: {temp_video_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating temporary file: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"임시 파일 생성에 실패했습니다: {e}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(temp_video_path)\n",
    "    if not cap.isOpened():\n",
    "        os.unlink(temp_video_path)\n",
    "        print(\"Cannot open uploaded video file or file is corrupted\")\n",
    "        raise HTTPException(status_code=400, detail=\"업로드된 비디오 파일을 열 수 없거나 손상되었습니다.\")\n",
    "\n",
    "    time_series_data = []\n",
    "    frame_number = 0\n",
    "    smooth_head_yaw, smooth_head_pitch = None, None\n",
    "    smooth_gaze_yaw, smooth_gaze_pitch = None, None\n",
    "    alpha = 0.2\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"End of video reached or read error\")\n",
    "                break\n",
    "\n",
    "            frame_number += 1\n",
    "            if frame_number % 10 != 0:\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing frame #{frame_number}\")\n",
    "\n",
    "            head_yaw, head_pitch, gaze_yaw, gaze_pitch = None, None, None, None\n",
    "            \n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results_facemesh = face_mesh.process(rgb_frame)\n",
    "            if results_facemesh.multi_face_landmarks:\n",
    "                face_landmarks = results_facemesh.multi_face_landmarks[0]\n",
    "                img_h, img_w, _ = frame.shape\n",
    "                face_3d_model = np.array([[0.0, 0.0, 0.0],\n",
    "                                         [0.0, -330.0, -65.0],\n",
    "                                         [-225.0, 170.0, -135.0],\n",
    "                                         [225.0, 170.0, -135.0],\n",
    "                                         [-150.0, -150.0, -125.0],\n",
    "                                         [150.0, -150.0, -125.0]], dtype=np.float64)\n",
    "                landmark_idx = [1, 152, 263, 33, 291, 61]\n",
    "                face_2d_points = np.array(\n",
    "                    [[face_landmarks.landmark[idx].x * img_w, face_landmarks.landmark[idx].y * img_h] for idx in landmark_idx],\n",
    "                    dtype=np.float64)\n",
    "                focal_length = img_w\n",
    "                cam_matrix = np.array([[focal_length, 0, img_w / 2],\n",
    "                                       [0, focal_length, img_h / 2],\n",
    "                                       [0, 0, 1]])\n",
    "                dist_coeffs = np.zeros((4, 1), dtype=np.float64)\n",
    "                success_pose, rot_vec, trans_vec = cv2.solvePnP(face_3d_model, face_2d_points, cam_matrix, dist_coeffs)\n",
    "                if not success_pose:\n",
    "                    print(f\"solvePnP failed in frame {frame_number}\")\n",
    "                else:\n",
    "                    rot_mat, _ = cv2.Rodrigues(rot_vec)\n",
    "                    sy = np.sqrt(rot_mat[0, 0] ** 2 + rot_mat[1, 0] ** 2)\n",
    "                    singular = sy < 1e-6\n",
    "                    if not singular:\n",
    "                        x, y, _ = np.arctan2(rot_mat[2, 1], rot_mat[2, 2]), np.arctan2(-rot_mat[2, 0], sy), np.arctan2(rot_mat[1, 0], rot_mat[0, 0])\n",
    "                    else:\n",
    "                        x, y, _ = np.arctan2(-rot_mat[1, 2], rot_mat[1, 1]), np.arctan2(-rot_mat[2, 0], sy), 0\n",
    "                    head_pitch = -np.degrees(x)\n",
    "                    head_yaw = -np.degrees(y)\n",
    "                    print(f\"Frame {frame_number} raw head_pitch: {head_pitch:.2f}, head_yaw: {head_yaw:.2f}\")\n",
    "\n",
    "            results_gaze = gaze_pipeline.step(frame)\n",
    "            if results_gaze and results_gaze.pitch is not None and len(results_gaze.pitch) > 0:\n",
    "                gaze_pitch = results_gaze.pitch[0]\n",
    "                gaze_yaw = results_gaze.yaw[0]\n",
    "                print(f\"Frame {frame_number} raw gaze_pitch: {gaze_pitch:.2f}, gaze_yaw: {gaze_yaw:.2f}\")\n",
    "\n",
    "            if head_yaw is not None and gaze_yaw is not None:\n",
    "                if smooth_head_yaw is None:\n",
    "                    smooth_head_yaw, smooth_head_pitch = head_yaw, head_pitch\n",
    "                    smooth_gaze_yaw, smooth_gaze_pitch = gaze_yaw, gaze_pitch\n",
    "                else:\n",
    "                    smooth_head_yaw = alpha * head_yaw + (1 - alpha) * smooth_head_yaw\n",
    "                    smooth_head_pitch = alpha * head_pitch + (1 - alpha) * smooth_head_pitch\n",
    "                    smooth_gaze_yaw = alpha * gaze_yaw + (1 - alpha) * smooth_gaze_yaw\n",
    "                    smooth_gaze_pitch = alpha * gaze_pitch + (1 - alpha) * smooth_gaze_pitch\n",
    "\n",
    "                frame_data = {\n",
    "                    \"frame\": frame_number,\n",
    "                    \"head_yaw\": float(smooth_head_yaw),\n",
    "                    \"head_pitch\": float(smooth_head_pitch),\n",
    "                    \"gaze_yaw\": float(smooth_gaze_yaw),\n",
    "                    \"gaze_pitch\": float(smooth_gaze_pitch)\n",
    "                }\n",
    "                print(f\"Frame {frame_number} smoothed data: {frame_data}\")\n",
    "                time_series_data.append(frame_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during video analysis: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"비디오 분석 중 오류가 발생했습니다: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        print(\"VideoCapture released\")\n",
    "        try:\n",
    "            os.unlink(temp_video_path)\n",
    "            print(f\"Temporary file deleted: {temp_video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting temporary file: {e}\")\n",
    "\n",
    "    scores = calc_score(time_series_data, calib_hy, calib_hp, calib_gy, calib_gp)\n",
    "    for frame_data, score in zip(time_series_data, scores):\n",
    "        frame_data[\"score\"] = score\n",
    "        print(f\"Frame {frame_data['frame']} score: {score}\")\n",
    "\n",
    "    average_score = float(np.mean(scores)) if scores else 0.0\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return JSONResponse(content={\n",
    "        \"time_series\": time_series_data,\n",
    "        \"average_score\": round(average_score)\n",
    "    })\n",
    "\n",
    "def calc_score(time_series, calib_hy, calib_hp, calib_gy, calib_gp):\n",
    "    scores = []\n",
    "    N = 9\n",
    "    K = 0.1\n",
    "    head_yaw_list, head_pitch_list, gaze_yaw_list, gaze_pitch_list = [], [], [], []\n",
    "    for idx, row in enumerate(time_series):\n",
    "        hy, hp, gy, gp = row[\"head_yaw\"], row[\"head_pitch\"], row[\"gaze_yaw\"], row[\"gaze_pitch\"]\n",
    "        head_yaw_list.append(hy)\n",
    "        head_pitch_list.append(hp)\n",
    "        gaze_yaw_list.append(gy)\n",
    "        gaze_pitch_list.append(gp)\n",
    "        if idx < N:\n",
    "            mean_hy = np.mean(head_yaw_list)\n",
    "            mean_hp = np.mean(head_pitch_list)\n",
    "            mean_gy = np.mean(gaze_yaw_list)\n",
    "            mean_gp = np.mean(gaze_pitch_list)\n",
    "        else:\n",
    "            mean_hy = np.mean(head_yaw_list[-N:])\n",
    "            mean_hp = np.mean(head_pitch_list[-N:])\n",
    "            mean_gy = np.mean(gaze_yaw_list[-N:])\n",
    "            mean_gp = np.mean(gaze_pitch_list[-N:])\n",
    "        abs_delta_hy = abs(hy - calib_hy)\n",
    "        abs_delta_hp = abs(hp - calib_hp)\n",
    "        abs_delta_gy = abs(gy - calib_gy)\n",
    "        abs_delta_gp = abs(gp - calib_gp)\n",
    "        rel_delta_hy = abs(hy - mean_hy)\n",
    "        rel_delta_hp = abs(hp - mean_hp)\n",
    "        rel_delta_gy = abs(gy - mean_gy)\n",
    "        rel_delta_gp = abs(gp - mean_gp)\n",
    "        w1, w2, w3, w4 = 0.25, 0.25, 0.25, 0.25\n",
    "        S = (\n",
    "            w1 * abs_delta_hy + w2 * abs_delta_hp + w3 * abs_delta_gy + w4 * abs_delta_gp +\n",
    "            w1 * rel_delta_hy + w2 * rel_delta_hp + w3 * rel_delta_gy + w4 * rel_delta_gp\n",
    "        )\n",
    "        score = int(100 * math.exp(-K * S))\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    print(\"Starting uvicorn server on 0.0.0.0:5003\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=5003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7db29f-1f4c-4583-91db-117f84c70fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
